---
title: "Storm Data Analysis - Week 4"
author: "Marco Baciarello"
date: "3/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, autodep = TRUE)

suppressMessages(library(tidyverse))
suppressMessages(library(dtplyr))
suppressMessages(library(data.table))
# library(skimr)
```

```{r function-definitions}

# WARNING: operates by reference, modifies argument data.table Adds columns for
# ranks based on event count, damage and injuries/fatalities. Useful to quickly
# re-rank tables with different elements or filters based on year.
rank_DT <- function(DT) {
    DT[, .(
            counter = .N, # Count elements in group
            # Total monetary damages per group
            total_dmg = sum(tot_dmg, na.rm = TRUE),
            # Total fatalities
            fatalities = sum(FATALITIES, na.rm = TRUE),
            # Injuries
            injuries = sum(INJURIES, na.rm = TRUE)
            ),
        # Grouping by event type
        by = event][
            ,
            # Compute a "percent rank": rank events by number of
            # occurrences (desc), then assign a percentile by
            # dividing rank by denominator (i.e. nrows)
            # Note double dot syntax to access calling scope
            `:=`(perc_rank = frankv(counter)/..denominator,
                 # Same for monetary damage
                 dmg_rank = frankv(total_dmg)/..denominator,
                 # And life damage
                 inj_rank = frankv(injuries)/..denominator,
                 fat_rank = frankv(fatalities)/..denominator
                 )
            ][
                # Filter for frequency rank >=90th
                perc_rank >= 0.9
            ][
                order(-counter)
            ]
}

# Min-max normalization
normalize <- function(x) {
    return ((x - min(x)) / (max(x) - min(x)))
  }

```

## Abstract

Blah blah

## Introduction

We analyze this, that and something else...

## Methods

The following code was used for analysis

## Data preparation

First, we download the data file and load it into an R object. We're using a `tidyr`-centric approach, leveraging on the full suite of library found in package `tidyverse`.

```{r data-prep, message=FALSE, warning=FALSE, cache = TRUE}

# Check if data dir exists -> create one
if (!file.exists("data")) {
    dir.create("./data")
}

# Get the data
if (!file.exists("data/stormdata.csv.bz2")) {
    download.file("https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2", "data/stormdata.csv.bz2")
}

# Read data in. fread seems way faster than other solutions.
rdata <- fread("data/stormdata.csv.bz2", data.table = FALSE)
# Convert to "lazy data.table" which will accept dplyr functions
data <- lazy_dt(rdata)

```

### Time formatting

We create 2 new table columns; one is a properly-formatted `POSIXct` column including date and time information from columns `BGN_DATE` and `BGN_TIME`. We will ignore end times of phenomena for now.

```{r column-formatting}

# TECH NOTE: We're using the `dtplyr` interface to use `tidyverse` grammar but
# run data.table code, for increased performance on large tables.

# Format data
fdata <- data %>%
    mutate(
        # Paste date and time For some reason, data includes a fixed time 00:00
        # which we don't need, so we split and get the first fragment
        temp_datetime = paste(strsplit(BGN_DATE, " ")[[1]][1], BGN_TIME),
        # Convert to datetime; 
        utc_datetime = as.POSIXct(
            temp_datetime,
            format = "%m/%d/%Y %H%M"),
        # correct timezones (there are older/unsupported abbreviations)
        c_timezone = recode(TIME_ZONE,
                            CDT = "US/Central",
                            CST = "US/Central",
                            UNK = "UTC",
                            MDT = "US/Mountain",
                            AST = "US/Eastern",
                            ADT = "US/Eastern",
                            CSt = "US/Central",
                            CSC = "UTC",
                            SCT = "US/Central",
                            ESY = "US/Eastern",
                            SST = "US/Samoa",
                            AKS = "US/Alaska",
                            GST = "US/Central"
                            ),
        # # Convert each time to its own time zone
        datetime = lubridate::force_tzs(utc_datetime, tzones = c_timezone),
        # Force POSIXct format, otherwise data won't be usable in data.table
        # package functions
        begin_time = as.POSIXct(datetime),
        # Finally, change EVTYPE into a factor ordered by frequency (may be
        # CPU-intensive)
        event = fct_infreq(as.factor(EVTYPE))
    ) %>%
    # Need to drop non-POSIXlt columns for data.table compatibility
    select(-contains("datetime")) %>%
    # End dtplyr "session," translate into data.table code and run
    as_tibble()


```

### Property and agricultural damages

A second column will be created in order to reconstruct information on property damage: columns `PROPDMGEXP` specifies an "exponent" for `PROPDMG`, in the form of a mathematical suffix: K for thousands, M for millions, and so on... We will compute a column containing the actual amount as an integer or double. Same operation will be performed for `CROPDMG` and `CROPDMGEXP`.

`*EXP` values are actually multipliers for variables indicating damages. The full legend can be found [here](ane.st/HandleExponent6c0f3 "Legend for EXP values").

```{r exponents}
unique(data$PROPDMGEXP)
unique(data$CROPDMGEXP)
```

We will transform them into the actual multiplier so we can compute actual damages. It is known that there are a few malformed values [*e.g.*: empty strings ("")]; they will be set to 0. According to the above analysis, malformed values are negligible both in numbers and absolute values.

```{r exp-to-multipliers}

# Cannot use dtplyr here because of error: Can't rename variables in this context.

temp_data <- fdata %>%
    mutate(
        year = as.integer(lubridate::year(begin_time)),
        prop_mult = case_when(
            PROPDMGEXP %in% c(0, 1, 2, 3, 4, 5, 6, 7, 8) ~ 10,
            PROPDMGEXP %in% c('m', 'M') ~ 1000000,
            PROPDMGEXP %in% c('k', 'K') ~ 1000,
            PROPDMGEXP %in% c('b', 'B') ~ 1000000000,
            PROPDMGEXP %in% c('h', 'H') ~ 100,
            PROPDMGEXP == '+' ~ 1,
            # Vector must be double because of limitations in `case_when`, so we
            # can't set a value to NA; use dummy code and convert later.
            TRUE ~ -999999
        ),
        crop_mult = case_when(
            PROPDMGEXP %in% c(0, 1, 2, 3, 4, 5, 6, 7, 8) ~ 10,
            CROPDMGEXP %in% c('m', 'M') ~ 1000000,
            CROPDMGEXP %in% c('k', 'K') ~ 1000,
            CROPDMGEXP %in% c('b', 'B') ~ 1000000000,
            CROPDMGEXP %in% c('h', 'H') ~ 100,
            CROPDMGEXP == '+' ~ 1,
            # Vector must be double because of limitations in `case_when`, so we
            # can't set a value to NA; use dummy code and convert later.
            TRUE ~ -999999
        ),
        # Change dummy value into NA
        across(ends_with("_mult"), ~na_if(., -999999)),
        crop_dmg = CROPDMG * crop_mult,
        prop_dmg = PROPDMG * prop_mult,
        tot_dmg = prop_dmg + crop_dmg
    )

# Switch back to dtplyr syntax and clean up
temp_data <- lazy_dt(temp_data)

# Filter out events which did not cause any injuries, fatalities or monetary
# damages
mdata <- temp_data %>% filter(
  (INJURIES > 0) | 
    (FATALITIES > 0) |
    (tot_dmg > 0)
) %>%
  # Apply data.table code
  as_tibble()

```
We can get the number of invalid `EXP` values resulting in `0`-value multipliers using the following code:

```{r invalid-exps}
# Property damage
inv_prop_exp <- filter(temp_data, PROPDMG != 0, prop_mult == 0) %>%
    select(PROPDMG, PROPDMGEXP, prop_mult, BGN_DATE) %>%
    nrow()

inv_crop_exp <- filter(temp_data, CROPDMG != 0, crop_mult == 0) %>%
    select(CROPDMG, CROPDMGEXP, crop_mult, BGN_DATE) %>%
    nrow()

# Clean up
rm(temp_data)
```

Using **inline code evaluation**, we can report `r inv_prop_exp` invalid property damage exponents, and `r inv_crop_exp` invalid crop damage exponents. Data from these was set to NA in order to compute means *etc...*

### Injuries and fatalities

These data do not seem to be needing any significant adjustments:

```{r inj-fat-describe}

skimr::skim(mdata$INJURIES)
cat("\n\n\n")
skimr::skim(mdata$FATALITIES)

```

### Selection of variables of interest

According to the requirements for this report, we will be describing associations between weather events and human and economical damages. After preparatory operations performed above, we will now drop variables which are not of interest to these analyses, in order to make data more easily readable and reduce resource requirements.

```{r data-column-thinning}

# Select useful columns
lean_data <- mdata %>%
    select(REFNUM, event, year, c_timezone,
           FATALITIES, INJURIES, tot_dmg)
# Economical damages aren't always available, so better have a tibble with all
# complete cases
dmg_data <- lean_data %>%
    filter(!is.na(tot_dmg))

```

### Selection of events

The vast majority of events have caused no damage to people or property. More than 90% of event *types* are not associated with damage, and many of them have occurred just once since 1950. This is mostly due to numerous, uncorrected mistakes made in data entry over the first ⅔ of the database's life.

We will now plot the distribution of values of interests, so as to only select significant *event types* in terms of occurrence. We'll work with the 90^th^ percentile (and above) of event frequency.

```{r evtype-selection}

# Make a copy of data and cast as DT
dt_data <- as.data.table(lean_data)
# Define the number of rows that will be in the grouped DT. This is equal to the
# unique values of event, as we'll group by that.
denominator <- uniqueN(dt_data, "event")
# Apply final transformations to DT
sel_data <- dt_data[, .(
                        counter = .N, # Count elements in group
                        # Total monetary damages per group
                        total_dmg = sum(tot_dmg, na.rm = TRUE),
                        # Total fatalities
                        fatalities = sum(FATALITIES, na.rm = TRUE),
                        # Injuries
                        injuries = sum(INJURIES, na.rm = TRUE)
                        ),
                    # Grouping by event type
                    by = event][
                        ,
                        # Compute a "percent rank": rank events by number of
                        # occurrences (desc), then assign a percentile by
                        # dividing rank by denominator (i.e. nrows)
                        # Note double dot syntax to access calling scope
                        `:=`(perc_rank = frankv(counter)/..denominator,
                             # Same for monetary damage
                             dmg_rank = frankv(total_dmg)/..denominator,
                             # And life damage
                             inj_rank = frankv(injuries)/..denominator,
                             fat_rank = frankv(fatalities)/..denominator
                             )
                        ][
                            # Filter for frequency rank >=95th
                            perc_rank >= 0.90
                        ][
                            order(-counter)
                        ]
                    
```

Figure 1 is a panel plot of data at or above the 95^th^ percentile of (A) event frequency by type, (B) monetary damages and (C) damages to persons in the period of interest (1950–2013). One can easily that the magnitude of event-related damages decreases almost exponentially with frequency. It should be noted that the preponderant majority of events below 95^th^ percentile of frequency are actually data-entry errors arising from "event type" being an open-text field in the database.

```{r frequency-plots}

sel_data %>% ggplot(
  aes(x = fct_reorder(event, perc_rank, .desc = T), y = counter)
) +
  geom_point() +
  scale_y_continuous(
    # limits = c(0, 100), 
    name = ("Frequency quantile\n")) +
  theme_bw()

```

