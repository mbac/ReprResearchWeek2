---
title: "Storm Data Analysis - Week 4"
author: "Marco Baciarello"
date: "3/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, autodep = TRUE)

suppressMessages(library(tidyverse))
suppressMessages(library(dtplyr))
suppressMessages(library(data.table))
# library(skimr)
```

```{r function-definitions}

# WARNING: operates by reference, modifies argument data.table Adds columns for
# ranks based on event count, damage and injuries/fatalities. Useful to quickly
# re-rank tables with different elements or filters based on year.
rank_DT <- function(DT) {
    DT[, .(
            counter = .N, # Count elements in group
            # Total monetary damages per group
            total_dmg = sum(tot_dmg, na.rm = TRUE),
            # Total fatalities
            fatalities = sum(FATALITIES, na.rm = TRUE),
            # Injuries
            injuries = sum(INJURIES, na.rm = TRUE)
            ),
        # Grouping by event type
        by = event][
            ,
            # Compute a "percent rank": rank events by number of
            # occurrences (desc), then assign a percentile by
            # dividing rank by denominator (i.e. nrows)
            # Note double dot syntax to access calling scope
            `:=`(perc_rank = frankv(counter)/..denominator,
                 # Same for monetary damage
                 dmg_rank = frankv(total_dmg)/..denominator,
                 # And life damage
                 inj_rank = frankv(injuries)/..denominator,
                 fat_rank = frankv(fatalities)/..denominator
                 )
            ][
                # Filter for frequency rank >=90th
                perc_rank >= 0.9
            ][
                order(-counter)
            ]
}

# Min-max normalization
normalizer <- function(x) {
    return ((x - min(x)) / (max(x) - min(x)))
  }

```

## Abstract

Blah blah

## Introduction

We analyze this, that and something else...

## Data Processing

First, we download the data file and load it into an R object. We're using a `tidyr`-centric approach, leveraging on the full suite of library found in package `tidyverse`.

```{r data-prep, message=FALSE, warning=FALSE, cache = TRUE}

# Check if data dir exists -> create one
if (!file.exists("data")) {
    dir.create("./data")
}

# Get the data
if (!file.exists("data/stormdata.csv.bz2")) {
    download.file("https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2", "data/stormdata.csv.bz2")
}

# Read data in. fread seems way faster than other solutions.
rdata <- fread("data/stormdata.csv.bz2", data.table = FALSE)
# Convert to "lazy data.table" which will accept dplyr functions
data <- lazy_dt(rdata)

```

### Time formatting

We create 2 new table columns; one is a properly-formatted `POSIXct` column including date and time information from columns `BGN_DATE` and `BGN_TIME`. We will ignore end times of phenomena for now.

```{r column-formatting}

# TECH NOTE: We're using the `dtplyr` interface to use `tidyverse` grammar but
# run data.table code, for increased performance on large tables.

# Format data
fdata <- data %>%
    mutate(
        # Paste date and time For some reason, data includes a fixed time 00:00
        # which we don't need, so we split and get the first fragment
        temp_datetime = paste(strsplit(BGN_DATE, " ")[[1]][1], BGN_TIME),
        # Convert to datetime; 
        utc_datetime = as.POSIXct(
            temp_datetime,
            format = "%m/%d/%Y %H%M"),
        # correct timezones (there are older/unsupported abbreviations)
        c_timezone = recode(TIME_ZONE,
                            CDT = "US/Central",
                            CST = "US/Central",
                            UNK = "UTC",
                            MDT = "US/Mountain",
                            AST = "US/Eastern",
                            ADT = "US/Eastern",
                            CSt = "US/Central",
                            CSC = "UTC",
                            SCT = "US/Central",
                            ESY = "US/Eastern",
                            SST = "US/Samoa",
                            AKS = "US/Alaska",
                            GST = "US/Central"
                            ),
        # # Convert each time to its own time zone
        datetime = lubridate::force_tzs(utc_datetime, tzones = c_timezone),
        # Force POSIXct format, otherwise data won't be usable in data.table
        # package functions
        begin_time = as.POSIXct(datetime),
        # Finally, change EVTYPE into a factor ordered by frequency (may be
        # CPU-intensive)
        event = fct_infreq(as.factor(str_to_lower(EVTYPE)))
    ) %>%
    # Need to drop non-POSIXlt columns for data.table compatibility
    select(-contains("datetime")) %>%
    # End dtplyr "session," translate into data.table code and run
    as_tibble()


```

### Property and agricultural damages

A second column will be created in order to reconstruct information on property damage: columns `PROPDMGEXP` specifies an "exponent" for `PROPDMG`, in the form of a mathematical suffix: K for thousands, M for millions, and so on... Values can also be integers, in which case they seem to indicate that the reported damage value is to be multiplied by 10 (see full legend [here](ane.st/HandleExponent6c0f3 "Legend for EXP values"). We will compute a column containing the actual amount as an integer or double. Same operation will be performed for `CROPDMG` and `CROPDMGEXP`.

```{r exponents}
unique(fdata$PROPDMGEXP)
unique(fdata$CROPDMGEXP)
```

We will transform them into the actual multiplier so we can compute actual damages. It is known that there are a few malformed values [*e.g.*: empty strings ("")]; they will be set to 0. According to the above analysis, malformed values are negligible both in numbers and absolute values.

```{r exp-to-multipliers}

# Cannot use dtplyr here because of error: Can't rename variables in this context.

temp_data <- fdata %>%
    mutate(
        year = as.integer(lubridate::year(begin_time)),
        prop_mult = case_when(
            PROPDMGEXP %in% c(0, 1, 2, 3, 4, 5, 6, 7, 8) ~ 10,
            PROPDMGEXP %in% c('m', 'M') ~ 1000000,
            PROPDMGEXP %in% c('k', 'K') ~ 1000,
            PROPDMGEXP %in% c('b', 'B') ~ 1000000000,
            PROPDMGEXP %in% c('h', 'H') ~ 100,
            PROPDMGEXP == '+' ~ 1,
            # Vector must be double because of limitations in `case_when`, so we
            # can't set a value to NA; use dummy code and convert later.
            TRUE ~ -999999
        ),
        crop_mult = case_when(
            PROPDMGEXP %in% c(0, 1, 2, 3, 4, 5, 6, 7, 8) ~ 10,
            CROPDMGEXP %in% c('m', 'M') ~ 1000000,
            CROPDMGEXP %in% c('k', 'K') ~ 1000,
            CROPDMGEXP %in% c('b', 'B') ~ 1000000000,
            CROPDMGEXP %in% c('h', 'H') ~ 100,
            CROPDMGEXP == '+' ~ 1,
            # Vector must be double because of limitations in `case_when`, so we
            # can't set a value to NA; use dummy code and convert later.
            TRUE ~ -999999
        ),
        # Change dummy value into NA
        across(ends_with("_mult"), ~na_if(., -999999)),
        crop_dmg = CROPDMG * crop_mult,
        prop_dmg = PROPDMG * prop_mult,
        tot_dmg = prop_dmg + crop_dmg
    )

# Switch back to dtplyr syntax and clean up
temp_data <- lazy_dt(temp_data)

# Filter out events which did not cause any injuries, fatalities or monetary
# damages
mdata <- temp_data %>% filter(
  (INJURIES > 0) | 
    (FATALITIES > 0) |
    (tot_dmg > 0)
) %>%
  # Apply data.table code
  as_tibble()

```

```{r invalid-exps, eval=FALSE, include=FALSE}

# DEBUG: this chunk not to be run nor shown when knitting.

# Property damage
inv_prop_exp <- filter(temp_data, PROPDMG != 0, prop_mult == -999999) %>%
  select(PROPDMG, PROPDMGEXP, prop_mult, BGN_DATE) %>%
  as.data.frame() %>%
  nrow()

inv_crop_exp <- filter(temp_data, CROPDMG != 0, crop_mult == -999999) %>%
  select(CROPDMG, CROPDMGEXP, crop_mult, BGN_DATE) %>%
  as.data.frame() %>%
  nrow()

# Clean up
# rm(temp_data)
```

### Injuries and fatalities

These data do not seem to be needing any significant adjustments:

```{r inj-fat-describe}

skimr::skim(mdata$INJURIES)
cat("\n\n\n")
skimr::skim(mdata$FATALITIES)

```

### Selection of variables of interest

According to the requirements for this report, we will be describing associations between weather events and human and economical damages. After preparatory operations performed above, we will now drop variables which are not of interest to these analyses, in order to make data more easily readable and reduce resource requirements.

```{r data-column-thinning}

# Select useful columns
lean_data <- mdata %>%
    select(REFNUM, event, year, c_timezone,
           FATALITIES, INJURIES, tot_dmg)

```

### Selection of events

The vast majority of events have caused no damage to people or property. More than 90% of event *types* are not associated with damage, and many of them have occurred just once since 1950. This is mostly due to numerous, uncorrected mistakes made in data entry over the first â…” of the database's life.

```{r evtype-selection}

# Make a copy of data and cast as DT
sel_data <- as.data.table(lean_data)
# Define the number of rows that will be in the grouped DT. This is equal to the
# unique values of event, as we'll group by that.
denominator <- uniqueN(fdata, "event")
# Make a copy for future use:
tot_data <- copy(sel_data)
# Apply final transformations to DT
sel_data[, .(
    counter = .N, # Count elements in group
    # Total monetary damages per group
    total_dmg = sum(tot_dmg, na.rm = TRUE),
    # Total fatalities
    fatalities = sum(FATALITIES, na.rm = TRUE),
    # Injuries
    injuries = sum(INJURIES, na.rm = TRUE)
  ),
  # Grouping by event type
  by = event][
    ,
    # Compute a "percent rank": rank events by number of
    # occurrences (desc), then assign a percentile by
    # dividing rank by denominator (i.e. nrows)
    # Note double dot syntax to access calling scope
    `:=`(perc_rank = frankv(counter)/..denominator,
         # Same for monetary damage
         dmg_rank = frankv(total_dmg)/..denominator,
         # And life damage
         inj_rank = frankv(injuries)/..denominator,
         fat_rank = frankv(fatalities)/..denominator
    )
  ][
    # Filter for frequency rank >=95th
    perc_rank >= 0.90
  ][
    order(-counter)
  ]

```

```{r ev-selection-comparison}

gdata <- as.data.table(temp_data)
# With DT syntax, group and compute relevant sums
totals <- gdata[,.(sum_inj = sum(INJURIES, na.rm = T),
               sum_fat = sum(FATALITIES, na.rm = T),
               sum_dmg = sum(tot_dmg, na.rm = T))]

selected <- sel_data[,.(sum_inj = sum(INJURIES, na.rm = T),
               sum_fat = sum(FATALITIES, na.rm = T),
               sum_dmg = sum(tot_dmg, na.rm = T))]

totals == selected
```

Events below the 90^th^ percentile of frequency are mostly irrelevant to the analysis. To confirm this, we calculated total damages, injuries and fatalities from the global data and from the selected data. Results are identical (see above), despite the latter table having only a fraction of the event types (`r length(unique(gdata$event))` *vs.* `r length(unique(sel_data$event))`).

### Aggregation of events by type

We then aggregate event types into arbitrary categories, both for clarity and because there is artificial variability in the form of spelling errors and variations (*e.g.*: `hail`, `marine hail`, `hail/wind`, `small hail` and so forth...)

```{r event-aggregation}

# Functions to aggregate (collapse) factor levels require specific values, specified in a named vector. Here, we use "data mining" to generate such vector.

# Create regexp patterns to match to. The following is also the priority order
# with which events will be assigned to a category.
new_fcts <- list(
  hail = c("hail"),
  tornado = c("torn", "nado"),
  tsunami = c("tsun"),
  stormy = c("storm*", "thunder*", "tstm", "rain", "precip", "hurric", "spout"),
  lightning = c("lightn"),
  flood = c("flood", "seiche"),
  cold = c("cold", "ice", "sleet", "winter", "snow", "blizz", "avalan"),
  heat = c("heat", "hot", "summ", "warm"),
  windy = c("wind", "devil", "b*rst"),
  maritime = c("sea*", "ocean*", "mari[nt]*",
               "curr*", "tid*", "surf*", "wav", "swell"),
  fog = c("fog"),
  drought = c("drought"),
  fire = c("fire", "blaze", "smok"),
  landslide = c("slide")
)

# Initialize list, vector and counter for the loop
old_events <- vector("list", 14)
new_events <- c()
i <- 1

suppressWarnings(
  # For every new event category
  for (i in seq(1:length(new_fcts))) {
    # Match old event instances
    old_instances <- str_subset(sel_data$event, new_fcts[[i]])
    # Add to list
    old_events[[i]] <- unique(old_instances)
    i <- i + 1
  }
)
# Name (tag) list elements
names(old_events) <- names(new_fcts)

# TODO: switch to using list names instead of manual entry of new factor levels
sel_data$event2 <- fct_collapse(
  sel_data$event,
  hail = old_events$hail,
  tornado = old_events$tornado,
  tsunami = old_events$tsunami,
  stormy = old_events$stormy,
  lightning = old_events$lightning,
  flood = old_events$flood,
  cold = old_events$cold,
  heat = old_events$heat,
  windy = old_events$windy,
  maritime = old_events$maritime,
  fog = old_events$fog,
  drought = old_events$drought,
  fire = old_events$fire,
  landslide = old_events$landslide,
  other_level = "Other events"
) %>%
  fct_recode(
    "Hail" = "hail",
    "Winds" = "windy",
    "Tornadoes" = "tornado",
    "Floods" = "flood",
    "Lightnings" = "lightning",
    "Cold weather" = "cold",
    "Storms/Rains" = "stormy",
    "Oceanic events" = "maritime",
    "Fires" = "fire",
    "Droughts" = "drought",
    "Hot weather" = "heat",
    "Fog" = "fog",
    "Landslides" = "landslide",
    "Tsunamis" = "tsunami"
  ) %>%
  # Reorder according to frequency, desc
  fct_infreq()

```

The choice of the fourteen categories was arbitrary, based on observation of event types appearing in the selected data. There are limitations to this approach, namely: events belonging to more than one category; and likely oversight of extremely rare events.

```{r memory-release-cleanup}

# Clean up large dataframes, no longer used
erase_us <- c("data", "fdata", "gdata", "mdata", "rdata", "tot_data")
rm(list = erase_us)
```

## Results

```{r overall-plot}

# Create plot data, use dtplyr for speed and convenience
pdata <- lazy_dt(sel_data)

# Plotting total instances of each event category
# pdata %>%
#     group_by(event2) %>%
#     summarise(
#         count = n(),
#         
#     )

```
